{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f207ad61-9871-43a1-836c-cb4b9aa2572f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "=> Use PySpark withColumnRenamed() to rename a DataFrame column, we often need to rename one column or multiple (or all) columns on PySpark DataFrame, you can do this in several ways. \n",
    "=> When columns are nested it becomes complicated.\n",
    "= Since DataFrame’s are an **immutable collection**,you can’t rename or update a column .\n",
    "=> while when we using withColumnRenamed() it creates a new DataFrame with updated column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4a1469cc-5f48-4b10-b9a3-4642e50bca8e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# UseCase1. PySpark \"withColumnRenamed\"– To rename DataFrame column name:_\n",
    "\n",
    "PySpark has a withColumnRenamed() function on DataFrame to change a column name. This is the most straight forward approach; this function takes two parameters; the first is your existing column name and the second is the new column name you wish for.\n",
    "\n",
    "# PySpark withColumnRenamed() Syntax:\n",
    "\n",
    "**withColumnRenamed(existingName, newName)**\n",
    "\n",
    "#output/result:-\n",
    "\n",
    "Returns a new DataFrame with a column renamed.\n",
    "\n",
    "#note:-\n",
    "\n",
    "withColumnRenamed function returns a new DataFrame and doesn’t modify the current DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "0548df05-31c7-4c16-a049-24487e260ad6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# UseCase2.PySpark withColumnRenamed – To rename multiple columns\n",
    "To change multiple column names, we should chain withColumnRenamed functions\n",
    "#code:- Renaming columns\n",
    "df2 = df.withColumnRenamed(\"dob\", \"DateOfBirth\") \\\n",
    "        .withColumnRenamed(\"salary\", \"salary_amount\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6a89ff2d-6f43-40c1-b3ae-e36a7b5c6930",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "3. Using PySpark StructType – To rename a nested column in Dataframe\n",
    "Changing a column name on nested data is not straight forward and we can do this by creating a new schema with new DataFrame columns using StructType and use it using cast function \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "caa115e8-a83f-4218-8230-15cdf574d794",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "4. Using Select – To rename nested elements.\n",
    "Let’s see another way to change nested columns by transposing the structure to flat.\n",
    "here use col funcation to selected structcolumn then use alias method then print schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c9cb3702-ac2e-4bd5-bfdf-50aa9b5e1f1c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# UseCase5. Using PySpark DataFrame withColumn – To rename nested columns\n",
    "When you have nested columns on PySpark DatFrame and if you want to rename it, use withColumn on a data frame object to create a new column from an existing and we will need to drop the existing column. Below example creates a “fname” column from “name.firstname” and drops the “name” column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bf45d4f2-e436-4790-af35-f2b7ca6531c1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 6. Using col() function – To Dynamically rename all or multiple columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "baa52b90-7419-47cf-a8be-95d058287ce2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Note\n",
    "# in usecase5To rename nested columns \n",
    "\n",
    " create new df then use df.withColumn(\"new_column_name\", col(\"existing_col_name\")) then drop existing column.\n",
    "\n",
    "4. Using Select – To rename nested elements.\n",
    "df.select(col(\"name.firstname\").alias(\"fname\").printSchema()\n",
    "Let’s see another way to change nested columns by transposing the structure to flat.\n",
    "here use col funcation to selected structcolumn then use alias method then print schema.\n",
    "\n",
    "# in usecase 6. Using col() function – To Dynamically rename all or multiple columns\n",
    "create new columne as list of col(\"existing_col_name\").alias(newcol_name))\n",
    "then create newdf as df6 = df.select(*newColumns)\n",
    "then print schema df6.printSchema()\n",
    "Each element of the newColumns list is a column expression created using col() function to reference specific columns from the DataFrame, with an alias assigned using the alias() method. This allows you to rename the columns while selecting them or creating new ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bb53a7b4-6389-463b-8dac-a74b4aeb51ad",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 7. Using toDF() – To change all columns in a PySpark DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5094ed14-90d2-4eea-86af-3c19346c81e7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- name: struct (nullable = true)\n |    |-- firstname: string (nullable = true)\n |    |-- middlename: string (nullable = true)\n |    |-- lastname: string (nullable = true)\n |-- dob: string (nullable = true)\n |-- gender: string (nullable = true)\n |-- salary: integer (nullable = true)\n\nroot\n |-- name: struct (nullable = true)\n |    |-- firstname: string (nullable = true)\n |    |-- middlename: string (nullable = true)\n |    |-- lastname: string (nullable = true)\n |-- DateOfBirth: string (nullable = true)\n |-- gender: string (nullable = true)\n |-- salary: integer (nullable = true)\n\nroot\n |-- name: struct (nullable = true)\n |    |-- firstname: string (nullable = true)\n |    |-- middlename: string (nullable = true)\n |    |-- lastname: string (nullable = true)\n |-- DateOfBirth: string (nullable = true)\n |-- gender: string (nullable = true)\n |-- salary_amount: integer (nullable = true)\n\nroot\n |-- name: struct (nullable = true)\n |    |-- fname: string (nullable = true)\n |    |-- middlename: string (nullable = true)\n |    |-- lname: string (nullable = true)\n |-- dob: string (nullable = true)\n |-- gender: string (nullable = true)\n |-- salary: integer (nullable = true)\n\nroot\n |-- fname: string (nullable = true)\n |-- mname: string (nullable = true)\n |-- lname: string (nullable = true)\n |-- dob: string (nullable = true)\n |-- gender: string (nullable = true)\n |-- salary: integer (nullable = true)\n\nroot\n |-- dob: string (nullable = true)\n |-- gender: string (nullable = true)\n |-- salary: integer (nullable = true)\n |-- fname: string (nullable = true)\n |-- mname: string (nullable = true)\n |-- lname: string (nullable = true)\n\nroot\n |-- fname: string (nullable = true)\n |-- mname: string (nullable = true)\n |-- lname: string (nullable = true)\n |-- DateOfBirth: string (nullable = true)\n |-- sex: string (nullable = true)\n |-- income: integer (nullable = true)\n\nroot\n |-- newCol1: struct (nullable = true)\n |    |-- firstname: string (nullable = true)\n |    |-- middlename: string (nullable = true)\n |    |-- lastname: string (nullable = true)\n |-- newCol2: string (nullable = true)\n |-- newCol3: string (nullable = true)\n |-- newCol4: integer (nullable = true)\n\n+--------------------+----------+------+------+\n|                name|       dob|gender|salary|\n+--------------------+----------+------+------+\n|    {James, , Smith}|1991-04-01|     M|  3000|\n|   {Michael, Rose, }|2000-05-19|     M|  4000|\n|{Robert, , Williams}|1978-09-05|     M|  4000|\n|{Maria, Anne, Jones}|1967-12-01|     F|  4000|\n|  {Jen, Mary, Brown}|1980-02-17|     F|    -1|\n+--------------------+----------+------+------+\n\n"
     ]
    }
   ],
   "source": [
    "# Example Data\n",
    "dataDF = [\n",
    "    (('James', '', 'Smith'), '1991-04-01', 'M', 3000),\n",
    "    (('Michael', 'Rose', ''), '2000-05-19', 'M', 4000),\n",
    "    (('Robert', '', 'Williams'), '1978-09-05', 'M', 4000),\n",
    "    (('Maria', 'Anne', 'Jones'), '1967-12-01', 'F', 4000),\n",
    "    (('Jen', 'Mary', 'Brown'), '1980-02-17', 'F', -1)\n",
    "]\n",
    "\n",
    "# Define the schema with nested structure\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "schema = StructType([\n",
    "    StructField('name', StructType([\n",
    "        StructField('firstname', StringType(), True),\n",
    "        StructField('middlename', StringType(), True),\n",
    "        StructField('lastname', StringType(), True)\n",
    "    ])),\n",
    "    StructField('dob', StringType(), True),\n",
    "    StructField('gender', StringType(), True),\n",
    "    StructField('salary', IntegerType(), True)\n",
    "])\n",
    "\n",
    "# Create a Spark session and DataFrame\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
    "df = spark.createDataFrame(data = dataDF, schema = schema)\n",
    "df.printSchema()\n",
    "\n",
    "# 1. PySpark withColumnRenamed – To rename a DataFrame column name\n",
    "df.withColumnRenamed(\"dob\", \"DateOfBirth\").printSchema()\n",
    "\n",
    "# 2. PySpark withColumnRenamed – To rename multiple columns\n",
    "df2 = df.withColumnRenamed(\"dob\", \"DateOfBirth\") \\\n",
    "    .withColumnRenamed(\"salary\", \"salary_amount\")\n",
    "df2.printSchema()\n",
    "\n",
    "# 3. Using PySpark StructType – To rename a nested column in DataFrame\n",
    "from pyspark.sql.functions import col\n",
    "schema2 = StructType([\n",
    "    StructField(\"fname\", StringType(), True),\n",
    "    StructField(\"middlename\", StringType(), True),\n",
    "    StructField(\"lname\", StringType(), True)\n",
    "])\n",
    "\n",
    "df.select(col(\"name\").cast(schema2), col(\"dob\"), col(\"gender\"), col(\"salary\")).printSchema()\n",
    "\n",
    "# 4. Using Select – To rename nested elements\n",
    "df.select(col(\"name.firstname\").alias(\"fname\"), \n",
    "          col(\"name.middlename\").alias(\"mname\"), \n",
    "          col(\"name.lastname\").alias(\"lname\"), \n",
    "          col(\"dob\"), col(\"gender\"), col(\"salary\")).printSchema()\n",
    "\n",
    "# 5. Using PySpark DataFrame withColumn – To rename nested columns\n",
    "df4 = df.withColumn(\"fname\", col(\"name.firstname\")) \\\n",
    "        .withColumn(\"mname\", col(\"name.middlename\")) \\\n",
    "        .withColumn(\"lname\", col(\"name.lastname\")) \\\n",
    "        .drop(\"name\")\n",
    "df4.printSchema()\n",
    "\n",
    "# 6. Using col() function – To Dynamically rename all or multiple columns\n",
    "newColumns = [col(\"name.firstname\").alias(\"fname\"),\n",
    "              col(\"name.middlename\").alias(\"mname\"),\n",
    "              col(\"name.lastname\").alias(\"lname\"),\n",
    "              col(\"dob\").alias(\"DateOfBirth\"),\n",
    "              col(\"gender\").alias(\"sex\"),\n",
    "              col(\"salary\").alias(\"income\")]\n",
    "df6 = df.select(*newColumns)\n",
    "df6.printSchema()\n",
    "\n",
    "# 7. Using toDF() – To change all columns in a PySpark DataFrame\n",
    "newColumns = [\"newCol1\",\"newCol2\",\"newCol3\",\"newCol4\"]\n",
    "df.toDF(*newColumns).printSchema() #(*newColumns) error withoutbracket\n",
    "df.show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "PySpark withColumnRenamed – To rename DataFrame column name",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
